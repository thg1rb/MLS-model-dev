{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### กลุ่ม Diabetes Prediction \n",
    "1. 6610402205 นายรักษิต รุ่งรัตนไชย หมู่ 1\n",
    "2. 6610402132 นายบวรรัตน์ ตั้งนรารัชชกิจ หมู่ 1\n",
    "3. 6610401985 นายไชยวัตน์ หนูวัฒนา หมู่ 1 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### วัตถุประสงค์ของระบบต้นแบบ\n",
    "กลุ่มของพวกเราเล็งเห็นถึงความสำคัญทางด้านสุขภาพของประชาชนคนไทย ในปัจจุบันคนไทยประสบกับปัญหาสุขภาพมากมายแต่หนึ่งปัญหาที่สร้างความเสียหายให้กับสุขภาพและคนไทยเป็นกันมากที่สุดนั่นคือโรคเบาหวาน พฤติกรรมการบริโภคของคนไทยในสมัยนี้ก็เป็นปัจจัยหนึ่งที่ทำให้ปริมาณผู้ป่วยโรคเบาหวานเพิ่มมากยิ่งขึ้น พวกเราจึงอยากทำระบบต้นแบบที่ช่วยแบ่งเบาภาระของเจ้าหน้าที่ในหน่วยงานสาธารณะสุข หมอ พยาบาล ในการวินิจฉัยโรคเบาหวาน ระบบต้นแบบอาจจะไม่สามารถชี้ชัดได้ 100 เปอเซนแต่สามารถคัดกรองผู้ป่วยและแบ่งเบาภาระได้ส่วนหนึ่งซึ่งพวกเราคิดว่ามันจะเป็นประโยชน์อย่างมากในการคัดกรองผู้ป่วย"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ลิงค์ไปยังข้อมูลที่จะใช้ในระบบต้นแบบ\n",
    "Link to data: https://www.kaggle.com/datasets/alexteboul/diabetes-health-indicators-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "diabetes = pd.read_csv(\"diabetes_binary_health_indicators_BRFSS2015.csv\")\n",
    "diabetes.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ปั​ญห​า ML ของตั​วเอ​งเป็นปั​ญหา​ป​ระเภ​ทใด (classificati​on/regres​sion/clust​ering/ge​neration)\n",
    "ปัญหา ML ของกลุ่มพวกเราเป็นปัญหาแบบ classification binary เนื่องจากเป็นการทำนายว่าข้อมูลของบุคคลที่นำเข้ามาในโมเดลนั้นเป็นหรือไม่เป็นโรคเบาหวานจึงเป็นการ classification binary อย่างชัดเจน"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### f​eat​ur​es ที่ใช้ในการทำนา​ยมีอะ​ไรบ้าง\n",
    "ก่อนที่เราจะสามารถดูค่าความสำคัญของแต่ละฟีเจอร์ได้นั้น เราต้องทำ Feature Engineer กับข้อมูลให้เรียบร้อยก่อนโดยมีขั้นตอนดังต่อไปนี้"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "แยกชุดข้อมูลออกเป็น ชุดข้อมูลเรียนรู้(train) ชุดข้อมูลทดสอบ(test) และชุดข้อมูลตรวจสอบความแม่นยำของโมเดล(validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(diabetes, stratify=diabetes[\"Diabetes_binary\"]\n",
    "                               , test_size=0.3, random_state=1234)\n",
    "test, validation = train_test_split(test, stratify=test[\"Diabetes_binary\"],\n",
    "                                    test_size=0.3, random_state=1234)\n",
    "len(train), len(test), len(validation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "เนื่องจากข้อมูลของกลุ่มเรานั้น ขาดความสมดุลอย่างมากสังเกตได้จาก จำนวนของ Label ที่มีจำนวนไม่เท่ากัน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "diabetes.Diabetes_binary.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ดังนั้นต้องทำให้ชุดข้อมูลที่นำมาใช้งานนั้นมีความสมดุลกันก่อนด้วยการทำ Under Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "resampler = RandomUnderSampler(random_state=1234)\n",
    "\n",
    "feature = diabetes.drop(columns=\"Diabetes_binary\").columns\n",
    "label = \"Diabetes_binary\"\n",
    "\n",
    "train[feature], train[label] = resampler.fit_resample(train[feature], train[label])\n",
    "train.dropna(inplace=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ขั้นตอนต่อมาคือการทำ Pipeline ของ Logistic Regression และ Decision Tree โดยเหตุผลที่เลือกทำ pipeline ทั้ง 2 โมเดลเพราะ ต้องการหาโมเดลที่มีความแม่นยำสูงและไม่ Overfit จนเกินไป"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้าง Pipeline ของ Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def create_pipline_logistic():\n",
    "    return make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"quantile\"), [\"BMI\"]),\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"uniform\"), [\"PhysHlth\"]),\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"uniform\"), [\"MentHlth\"]),\n",
    "            remainder=\"passthrough\"\n",
    "        ),\n",
    "        LogisticRegression(max_iter=500)\n",
    "    )\n",
    "\n",
    "logistic_pipeline = create_pipline_logistic()\n",
    "logistic_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้างโมเดล Logistic Regression และตรวจสอบความแม่นยำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.fit(train[feature], train[label])\n",
    "logistic_pipeline.score(validation[feature], validation[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้าง Pipeline ของ SDG Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "def create_pipline_sgd():\n",
    "    return make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"quantile\"), [\"BMI\"]),\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"uniform\"), [\"PhysHlth\"]),\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"uniform\"), [\"MentHlth\"]),\n",
    "            remainder=\"passthrough\"\n",
    "        ),\n",
    "        SGDClassifier()\n",
    "    )\n",
    "\n",
    "sgd_pipline = create_pipline_sgd()\n",
    "sgd_pipline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้างโมเดล SDG Classifier และตรวจสอบความแม่นยำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipline.fit(train[feature], train[label])\n",
    "sgd_pipline.score(validation[feature], validation[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้าง Pipeline ของ Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "def create_pipline_decisiontree():\n",
    "    return make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"quantile\"), [\"BMI\"]),\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"uniform\"), [\"PhysHlth\"]),\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"uniform\"), [\"MentHlth\"]),\n",
    "            remainder=\"passthrough\"\n",
    "        ),\n",
    "        DecisionTreeClassifier()\n",
    "    )\n",
    "\n",
    "decision_pipeline = create_pipline_decisiontree()\n",
    "decision_pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "สร้างโมเดล Decision Tree และตรวจสอบความแม่นยำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_pipeline.fit(train[feature], train[label])\n",
    "decision_pipeline.score(validation[feature], validation[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ตรวจสอบค่าความสำคัญของแต่ละฟีเจอร์จากโมเดล Logistic Regression เพราะเป็นโมเดลมีความแม่นยำสูงที่สุด"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "importance = permutation_importance(logistic_pipeline, test[feature], test[label], random_state=1234)\n",
    "importance = pd.Series(importance.importances_mean, index=feature)\n",
    "importance.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "โดยเลือก 7 ฟีเจอร์แรกที่มีค่าความสำคัญสูงสุดมาใช้งาน"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance_features = [\"GenHlth\", \"BMI\", \"HeartDiseaseorAttack\", \"DiffWalk\", \"HvyAlcoholConsump\", \"Stroke\", \"NoDocbcCost\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### เฉ​ลย (label) ที่ต้​องกา​รทำนา​ยคื​ออ​ะไร\n",
    "สิ่งที่ต้องการทำนายคือ 0 หรือ 1 (0 คือไม่เป็นโรคเบาหวานและ 1 คือเป็นโรคเบาหวาน)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"ไม่เป็นโรคเบาหวาน\", \"เป็นโรคเบาหวาน\"]\n",
    "\n",
    "for i in diabetes.Diabetes_binary.unique().astype(int):\n",
    "    print(f\"{i} = {labels[i]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ระบบของตัวเองสาม​ารถใช้ al​gorith​m อะไรได้บ้าง เพ​ราะเห​ตุใด\n",
    "\n",
    "เกณฑ์ที่กลุ่มเราใช้ในการเลือกใช้โมเดลจะอิงจาก Cheat Sheet ของ Scikit-Learn ดังภาพต่อไปนี้:\n",
    "\n",
    "<img src=\"https://scikit-learn.org/1.3/_static/ml_map.png\" alt=\"sklearn-cheat-sheet\" width=\"700\">\n",
    "\n",
    "เนื่องจากชุดข้อมูลที่กลุ่มเรานำมาใช้นั้นเป็นแบบ Classification หรือแบ่งแยกประเภท และมีเป็นข้อมูลที่มี Labeled จากรูปจะมี `SGD Classifier` และ `Kernel Approximation` ที่สามารถนำมาใช้กับข้อมูลของกลุ่มเราได้ และยังมี Algorithm อื่นๆอีกที่สามารถนำมาใช้งานกับข้อมูลของกลุ่มเราได้แต่ไม่ได้อยู่ใน Cheat Sheet นี้เช่น `Logistic Regression` และ `Decision Tree` เป็นต้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### แต่ละ algor​ithm มี hype​rpar​am​eter อะไ​รบ้าง ปรับเป็​นอะไรไ​ด้บ้าง\n",
    "แต่ละ algorithm มี hyperparameter ดังต่อไปนี้"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sgd_pipline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทำการ Tunning ให้กับโมเดล Logistic Regresion เพื่อหา hyperparameter ที่ทำให้โมเดลมีประสิทธิภาพมากขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "pipeline = create_pipline_logistic() \n",
    "\n",
    "param_dist = {\n",
    "     'columntransformer__kbinsdiscretizer-1__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "     'columntransformer__kbinsdiscretizer-2__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "     'columntransformer__kbinsdiscretizer-3__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "     'columntransformer__kbinsdiscretizer-1__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "     'columntransformer__kbinsdiscretizer-2__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "     'columntransformer__kbinsdiscretizer-3__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "     'columntransformer__kbinsdiscretizer-1__n_bins': [5, 10, 20],\n",
    "    'columntransformer__kbinsdiscretizer-2__n_bins': [5, 10, 20],\n",
    "    'columntransformer__kbinsdiscretizer-3__n_bins': [5, 10, 20],\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'logisticregression__max_iter': [100, 200, 300],\n",
    "    'logisticregression__tol': [1e-4, 1e-3, 1e-2]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_dist, n_iter=10, random_state=0)\n",
    "search.fit(train[feature], train[label])\n",
    "search.best_params_, f\"{search.best_score_:.3f}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ปรับแต่ง hyperparameter ตามผลที่ได้จากการ tunning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทำการ Tunning ให้กับโมเดล SGD Classifier เพื่อหา hyperparameter ที่ทำให้โมเดลมีประสิทธิภาพมากขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pipeline = create_pipline_sgd()\n",
    "\n",
    "param_dist = {\n",
    "    'columntransformer__kbinsdiscretizer-1__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "    'columntransformer__kbinsdiscretizer-2__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "    'columntransformer__kbinsdiscretizer-3__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "    'columntransformer__kbinsdiscretizer-1__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "    'columntransformer__kbinsdiscretizer-2__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "    'columntransformer__kbinsdiscretizer-3__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "    'columntransformer__kbinsdiscretizer-1__n_bins': [5, 10, 20],\n",
    "    'columntransformer__kbinsdiscretizer-2__n_bins': [5, 10, 20],\n",
    "    'columntransformer__kbinsdiscretizer-3__n_bins': [5, 10, 20],\n",
    "    'sgdclassifier__loss' : ['hinge', 'log_loss', 'modified_huber', 'squared_hinge', 'perceptron', 'squared_error', 'huber', 'epsilon_insensitive', 'squared_epsilon_insensitive'],\n",
    "    'sgdclassifier__penalty': ['l2', 'l1', 'elasticnet', None]\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_dist, n_iter=20, random_state=0)\n",
    "search.fit(train[feature], train[label])\n",
    "search.best_params_, f\"{search.best_score_:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_.score(validation[feature], validation[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ทำการ Tunning ให้กับโมเดล Decision Tree เพื่อหา hyperparameter ที่ทำให้โมเดลมีประสิทธิภาพมากขึ้น"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pipeline = create_pipline_decisiontree()\n",
    "\n",
    "param_dist = {\n",
    "    'columntransformer__kbinsdiscretizer-1__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "    'columntransformer__kbinsdiscretizer-2__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "    'columntransformer__kbinsdiscretizer-3__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "    'columntransformer__kbinsdiscretizer-1__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "    'columntransformer__kbinsdiscretizer-2__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "    'columntransformer__kbinsdiscretizer-3__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "    'columntransformer__kbinsdiscretizer-1__n_bins': [5, 10, 20],\n",
    "    'columntransformer__kbinsdiscretizer-2__n_bins': [5, 10, 20],\n",
    "    'columntransformer__kbinsdiscretizer-3__n_bins': [5, 10, 20],\n",
    "    'decisiontreeclassifier__criterion':['gini', 'entropy', 'log_loss'],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_dist, n_iter=20, random_state=34)\n",
    "search.fit(train[feature], train[label])\n",
    "search.best_params_, f\"{search.best_score_:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_.score(validation[feature], validation[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### คำตอบที่ได้จากการทำนายของ model เบื้องต้นปกติหรือไม่ อย่างไร ต้องมีการ p​os​t-p​roc​ess หรือไม่ อย่างไร\n",
    "เริ่มด้วยการ\n",
    "1. ทดสอบประสิทธิภาพของโมเดลเบื้องต้น \n",
    "2. นำโมเดลเบื้องต้นมาทำนายเพื่อดูความแม่นยำ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_efficient_logistic = create_pipline_logistic()\n",
    "%time test_efficient_logistic.fit(train[feature], train[label])\n",
    "test_efficient_logistic.score(validation[feature], validation[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = validation.sample(5, random_state=1234)\n",
    "samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted = logistic_pipeline.predict(samples)\n",
    "predicted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ผลจากการทำนายเบื้องต้นพบว่ายังคงมีการทำนายผิดพลาดเกิดขึ้นอยู่จึงต้องทำการ post process เพื่อเพิ่มความแม่นยำของโมเดลโดยการ drop บางฟีเจอร์ที่มีความสำคัญน้อยออกไปใช้เฉพาะฟีเจอร์ที่มีความสำคัญมาก (ดูความสำคัญของฟีเจอร์จากการทำ feature importance ก่อนหน้านี้)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def create_new_pipline_logistic():\n",
    "    return make_pipeline(\n",
    "        make_column_transformer(\n",
    "            (KBinsDiscretizer(encode=\"ordinal\", strategy=\"quantile\"), [\"BMI\"]),\n",
    "            remainder=\"passthrough\"\n",
    "        ),\n",
    "        LogisticRegression(max_iter=500)\n",
    "    )\n",
    "\n",
    "new_logistic_pipeline = create_new_pipline_logistic()\n",
    "new_logistic_pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_logistic_pipeline.get_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time new_logistic_pipeline.fit(train[importance_features], train[label])\n",
    "new_logistic_pipeline.score(validation[importance_features], validation[label])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pipeline = create_new_pipline_logistic() \n",
    "\n",
    "param_dist = {\n",
    "     'columntransformer__kbinsdiscretizer__strategy': ['uniform', 'quantile', 'kmeans'],\n",
    "     'columntransformer__kbinsdiscretizer__encode': ['ordinal', 'onehot', 'onehot-dense'],\n",
    "     'columntransformer__kbinsdiscretizer__n_bins': [5, 10, 20],  # Adjusted to correct key\n",
    "    'logisticregression__C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'logisticregression__penalty': ['none', 'l1', 'l2', 'elasticnet'],\n",
    "    'logisticregression__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'logisticregression__max_iter': [100, 200, 300],\n",
    "    'logisticregression__tol': [1e-4, 1e-3, 1e-2],\n",
    "}\n",
    "\n",
    "search = RandomizedSearchCV(pipeline, param_dist, n_iter=10, random_state=0)\n",
    "search.fit(train[importance_features], train[label])\n",
    "search.best_params_, f\"{search.best_score_:.3f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "search.best_estimator_.score(validation[importance_features], validation[label])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "หลังจากการทำ post process โดยการทำ Hyperparameters Tuning พบว่า score ความแม่นยำเพิ่มขึ้นเล็กน้อยและ มีระยะเวลาที่ใช้ในการสร้างโมเดลที่เร็วขึ้น"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ปร​ะเด็น​อื่น ๆ ที่เกี่ยว​ข้องกั​บข้อมูลของ​ตัวเอง\n",
    "\n",
    "* ในขั้นตอนการหาค่าความสำคัญของฟีเจอร์จากการทดสอบหาค่าความสำคัญจากโมเดล SDG Classifier พบว่าผลลัพธ์ที่ได้มีค่าเป็น 0.0 ทั้งหมด ไม่ทราบว่าเพราะเหตุผลใดจึงได้ผลลัพธ์ออกมาเป็นแบบนั้นจึงเลือกที่จะใช้แค่ 2 โมเดลนั่นคือ Logistic regression และ Decision tree ซึ่งทั้งสองโมเดลนี้สามารถหา feature importance ได้\n",
    "\n",
    "* อีกประเด็นที่ค้นพบก็คือในตอนแรกข้อมูลเกิดความ imbalance ซึ่งคิดว่าไม่ได้ส่งผลอะไรเพราะตอนแบ่ง train test ได้ทำการแบ่งอย่างถูกต้องต้องแล้วตามที่งานก่อนหน้านี้ที่ได้ทำไปซึ่งผลการทดสอบโมเดลก็อยู่ในคะแนนที่ดีประมาณ 0.8 กว่าๆ แต่เมื่อทำการทดลองใช้โมเดลกับข้อมูลที่มีแต่คนที่เป็นโรคเบาหวานปรากฏว่าโมเดลทำนายไม่แม่นเลย score ลดลงเหลือ 0.1 กว่าๆแปลว่าโมเดลพยายามเดา label ให้เป็น 0 เพราะในชุด train มีจำนวน label 0 มากกว่า 1 หลายเท่าตัว จึงทำการแก้ imbalace โดยใช้วิธี `under sampling` ให้จำนวนคนเป็นกับไม่เป็นโรคเบาหวานเท่าๆกัน (ซึ่งแลกมาด้วยการที่ข้อมูลหายไปจำนวนมากจาก 2 แสนกว่า เหลือ 5 หมื่นกว่า) แต่สิ่งที่ได้มาคือโมเดลทำนายได้ดีขึ้นวัด score อยู่ที่ 0.7 นิดๆทั้งชุดที่มีแต่คนเป็นโรคและชุดของคนที่ไม่เป็นโรค (อัลกอริธึมที่ใช้คือ Logistic Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ก​าร​มีส่วน​ร่วมของสม​าชิกแต่ละ​คนในกลุ่ม (แต่ล​ะคนทำอะไรบ้าง)\n",
    "- นายรักษิต รุ่งรัตนไชย ทำหน้าที่เขียนในหัวข้อ 1 2 3 4 8 9 10\n",
    "- นายบวรรัตน์ ตั้งนรารัชชกิจ ทำหน้าที่เขียนในหัวข้อ 5 6 7 ทำการ tunning model เพื่อหา hyperparameter ที่ดีที่สุด\n",
    "- นายไชยวัตน์ หนูวัฒนา เขียน pipeline เพื่อใช้ในการสร้าง model, หา feature importance และ tunning hyperparameter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "### กา​รเปิ​ดเผ​ยกา​รใช้เค​รื่อ​งมือปัญ​ญาป​ระดิษฐ์ (ใช้อะไร ใช้เพื่ออะไร ใช้อย่างไร, pro​mpt อย่างไร)\n",
    "ในการทำงานครั้งนี้ ไม่ได้ใช้ปัญญาประดิษฐ์ในการทำงานแต่อย่างใด"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLS_01418262",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
